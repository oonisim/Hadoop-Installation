#--------------------------------------------------------------------------------
# Install Packages
# Use shell + tar instead of Ansible unarehive to use --strip-components=1 option.
#--------------------------------------------------------------------------------
- name: "Install {{ SPARK_PACKAGE }}"
  shell: |
    tar --overwrite --owner={{ SPARK_ADMIN }} --group={{ SPARK_GROUP }} -xzf {{ SPARK_PACKAGE }} -C {{ SPARK_HOME }} --strip-components=1
    chown -R {{ SPARK_ADMIN }}:{{ SPARK_GROUP }} {{ SPARK_HOME }}
  args:
    chdir: "{{ SPARK_DOWNLOAD_DIR }}"

#unarchive:
#    src: "{{ SPARK_DOWNLOAD_DIR }}/{{ SPARK_PACKAGE }}"
#    dest: "{{ SPARK_HOME }}"
#    owner: "{{ SPARK_ADMIN }}"
#    group: "{{ SPARK_GROUP }}"
#    remote_src: yes

#--------------------------------------------------------------------------------
# When Hadoop installation is not in the Spark server, make the hadoop/yarn
# configurations need to be available.
#--------------------------------------------------------------------------------
- name: "Copy configuration files"
  template:
    src   : "{{ item }}"
    dest  : "{{ SPARK_HOME }}/conf/{{ item | basename | regex_replace('\\.j2','') }}"
    owner : "{{ SPARK_ADMIN }}"
    group : "{{ SPARK_GROUP }}"
    mode  : 0664
    backup: yes
  with_fileglob:
    - "{{ role_path }}/templates/conf/*"

- name: "Copy pofile.d files"
  template:
    src   : "{{ item }}"
    dest  : "/etc/profile.d/{{ item | basename | regex_replace('\\.j2','') }}"
    owner : "{{ SPARK_ADMIN }}"
    group : "{{ SPARK_GROUP }}"
    mode  : 0664
    backup: yes
  with_fileglob:
    - "{{ role_path }}/templates/profile.d/*"